{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoL2BGEQPHhu"
   },
   "source": [
    "**Please select the T4-GPU runtime before proceeding, as the embedding generation model requires a GPU for fast inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj86XyApSMfT"
   },
   "source": [
    "# Project Overview\n",
    "\n",
    "---\n",
    "\n",
    "This project focuses on developing a Retrieval-Augmented Generation (RAG) based AI chatbot built on top of the Python Stack Overflow Question Answering Dataset. The chatbot is designed to answer any query related to the Python programming language. It matches user input queries with questions available in the dataset, retrieves relevant questions and metadata, and then passes the retrieved answers as context to a language model (LLM) to generate accurate responses.\n",
    "\n",
    "### Core Functionality\n",
    "\n",
    "The bot's main functionality involves generating embeddings of cleaned questions, which are stored in a vector database. When a user asks a question related to Python, the bot generates an embedding of the input query, matches it with questions stored in the database, and retrieves the answers to the top three matched questions. These answers are then passed to the LLM as context to generate an accurate response.\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "The project consists of three main pipelines:\n",
    "\n",
    "1. **Data Processing Pipeline:**\n",
    "   - **Cleaning and Transformation:** This pipeline is responsible for cleaning and transforming the data into the required format. It removes HTML tags to improve readability and context, handles null values, standardizes date formats, and performs text cleaning.\n",
    "   - **Storage:** The cleaned data is stored in an SQLite database for persistent storage. SQLite database is selected for simplicty according to the scope of this project. For questions with multiple answers, each answer is stored in a separate row, grouped by the question, to reduce redundancy and dataset size.\n",
    "   - **Optimization:** Parallel processing is used to clean and store data quickly, making the solution scalable for larger datasets. SQLite is used in WAL mode to handle data efficiently and in parallel. Indexes are created for efficient querying from the database.\n",
    "\n",
    "2. **Vector Database Pipeline:**\n",
    "   - **Embedding Generation:** This pipeline generates embeddings using an open-source small 450M parameter model and proprietary VoyageAI models. While the open-source model is good for testing, the VoyageAI model provides better results for production use.\n",
    "   - **Storage:** ChromaDB is used as the vector database. Batch processing is employed for efficient and fast embedding generation and upserting vectors into the database.\n",
    "\n",
    "3. **Inference Pipeline:**\n",
    "   - **Query Processing:** This pipeline receives user queries, generates embeddings using the same model used for dataset embedding generation, and queries the vector database for the top relevant questions.\n",
    "   - **Answer Retrieval:** It retrieves the question IDs from the metadata of the matched vectors and fetches the corresponding answers from the database. These answers are then passed as context to the LLM to generate the final response.\n",
    "\n",
    "By utilizing these pipelines, the chatbot can provide accurate and relevant answers to Python programming queries, enhancing the user's experience and ensuring reliable information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgGnQzdFuPQl"
   },
   "source": [
    "Please install the required dependencies. The kernel will automatically prompt you for a restart; kindly restart the kernel when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "AX37T6p2yIdl",
    "outputId": "6287bdfb-bc13-4b02-e68c-3607bf124af6"
   },
   "outputs": [],
   "source": [
    "# !pip install pinecone voyageai protoc_gen_openapiv2 numpy==1.24.4 langchain_chroma langchain_voyageai langchain_huggingface\n",
    "# exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "VoyageAI_API_KEY = os.getenv(\"VOYAGEAI_API_KEY\") \n",
    "hf_client_API_KEY = os.getenv(\"hf_clent_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J31pDG9pyGO6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "from typing import Dict, List, Any, Union, Tuple, Iterator\n",
    "import json\n",
    "import numpy as np\n",
    "from urllib.parse import unquote\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import csv\n",
    "import gdown\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9fNpG0hw65V"
   },
   "source": [
    "# **Stack Overflow Python Questions Dataset**\n",
    "A collection of Python-related questions and answers from Stack Overflow's. The dataset contains technical discussions covering Python programming topics. Dataset includes a mix of structured and unstructured data.\n",
    "Each entry includes:\n",
    "\n",
    "* Question title\n",
    "* Question Body\n",
    "* Question_Id\n",
    "* Answer content with code examples\n",
    "* Answer_id\n",
    "* Timestamps for posts\n",
    "* Community voting scores\n",
    "* Technical tags\n",
    "* Unique identifiers\n",
    "\n",
    "The data represents authentic programming challenges and solutions from the Python community during Stack Overflow's initial launch period, offering insights into early Python development practices and knowledge sharing.\n",
    "\n",
    "Originally, dataset is consists of more than 1 Million sample points, but for the simplicty of the problem i reduced it to 25000 sample points and store it into CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95Nw-kAGvBTl"
   },
   "source": [
    "This cell will download the CSV File from the google drive and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "tZ7YJyg31Lm0",
    "outputId": "94c7fbd3-f8c3-42d7-c0db-2485e1d5cdff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VuXXeiMlGn0utMZWrEXpDSkoqx8hC630\n",
      "To: /content/python_stackover_flow_dataset.csv\n",
      "100%|██████████| 41.2M/41.2M [00:00<00:00, 116MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/python_stackover_flow_dataset.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_path = \"/content/python_stackover_flow_dataset.csv\"\n",
    "url = 'https://drive.google.com/uc?id=1VuXXeiMlGn0utMZWrEXpDSkoqx8hC630'\n",
    "gdown.download(url, dataset_file_path, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h54I7IlJvlq8"
   },
   "source": [
    "Cell reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "jLdey-yi0Jql",
    "outputId": "630640b1-e808-46ae-bdff-f55d375cb50b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (25000, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 25000,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5276,\n        \"samples\": [\n          \"Adding a Method to an Existing Object Instance\",\n          \"Django: How do I create a generic url routing to views?\",\n          \"How can i use TurboMail 3 together with TurboGears 2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 255904,\n        \"min\": 469,\n        \"max\": 901061,\n        \"num_unique_values\": 5279,\n        \"samples\": [\n          972,\n          344826,\n          141647\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_body\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5278,\n        \"samples\": [\n          \"<p>I've read that it is possible to add a method to an existing object (e.g. not in the class definition) in <strong>Python</strong>, I think this is called <em>Monkey Patching</em> (or in some cases <em>Duck Punching</em>). I understand that it's not always a good decision to do so. But, how might one do this?</p>\\n\\n<p><strong>UPDATE 8/04/2008 00:21:01 EST:</strong></p>\\n\\n<p><a href=\\\"http://stackoverflow.com/a/982\\\">That</a> looks like a good answer John Downey, I tried it but it appears that it ends up being not a <em>true</em> method.</p>\\n\\n<p>Your example defines the new patch function with an argument of <strong><code>self</code></strong>, but if you write actual code that way, the now patched class method asks for an argument named <code>self</code> (it doesn't automagically recognize it as the object to which it is supposed to bind, which is what would happen if defined within the class definition), meaning you have to call <strong><code>class.patch(obj)</code></strong> instead of just <strong><code>class.patch()</code></strong> if you want the same functionality as a <em>true</em> method.</p>\\n\\n<p><strong>It looks like Python isn't really treating it as a method, but more just as a variable which happens to be a function</strong> (and as such is callable).  Is there any way to attach an actual method to a class?</p>\\n\\n<p>Oh, and Ryan, <a href=\\\"http://pypi.python.org/pypi/monkey\\\">that</a> isn't exactly what I was looking for (it isn't a builtin functionality), but it is quite cool nonetheless.</p>\\n\",\n          \"<p>Am using django and am implementing WMD on my site, am just wondering how do i convert the markdown syntax to HTML for display purposes, is there some sort of function i should call to do this conversion?</p>\\n\\n<p>What is the best way to handle markdown ie. do i save the markdown as is to the database then parse it when displaying it or should i save the converted HTML then convert it to markup during editing?</p>\\n\",\n          \"<p>I've written code for communication between my phone and comp thru TCP sockets. When I type out the code line by line in the interactive console it works fine. However, when i try running the script directly through filebrowser.py it just wont work. I'm using Nokia N95. Is there anyway I can run this script directly without using filebrowser.py?\\nAm new to Python for mobile phones, so any suggestions would be appreciated.\\nThanks</p>\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 337,\n        \"min\": -6,\n        \"max\": 5524,\n        \"num_unique_values\": 327,\n        \"samples\": [\n          647,\n          227,\n          1686\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5278,\n        \"samples\": [\n          \"2008-08-04T02:17:51Z\",\n          \"2009-04-18T07:48:08Z\",\n          \"2008-09-26T20:07:55Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9638516,\n        \"min\": 497,\n        \"max\": 40139977,\n        \"num_unique_values\": 25000,\n        \"samples\": [\n          5089930,\n          870003,\n          369763\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24995,\n        \"samples\": [\n          \"<p>New in Python 3.2, you can now use e.g. <code>code_info()</code> from the dis module:\\n<a href=\\\"http://docs.python.org/dev/whatsnew/3.2.html#dis\\\">http://docs.python.org/dev/whatsnew/3.2.html#dis</a></p>\\n\",\n          \"<p>I believe you mean having a Python script that tries to speak HTTP.\\nI suggest you to use a high-level library that handles cookies automatically.\\npycurl, mechanize, twill - you choose.</p>\\n\\n<p>For Nikhil Chelliah:</p>\\n\\n<p>I don't see what's not clear here.</p>\\n\\n<p><strong>Accepting</strong> a cookie happens client-side. The server can <strong>set</strong> a cookie.</p>\\n\",\n          \"<p>As @S.Lott 's say, STUN is your first choice protocol .</p>\\n\\n<p>And then, STUN is just a protocol.Here is my advice:</p>\\n\\n<p>1 <strong>STUN</strong> now has two version : the old version is <a href=\\\"http://www.ietf.org/rfc/rfc3489.txt\\\" rel=\\\"nofollow\\\">RFC3489</a>  - this is a lightweight protocol that allows applications to discover the presence and types of NATs and firewalls between them and the public Internet  (so it's mainly and only for detecting NAT type ); and the new version is <a href=\\\"http://www.ietf.org/rfc/rfc5389.txt\\\" rel=\\\"nofollow\\\">RFC5389</a> - this is a tool for other  protocols in dealing with NAT traversal . </p>\\n\\n<p>2 Also there is a relay extension to STUN named <strong>TURN</strong> <a href=\\\"http://www.ietf.org/rfc/rfc5766.txt\\\" rel=\\\"nofollow\\\">RFC5766</a>.  TURN allows the host to control the operation of the relay and to exchange packets with its peers using  the relay. TURN differs from some other relay control protocols in that it allows a client to communicate with multiple peers using a single relay address.</p>\\n\\n<p>The tools:</p>\\n\\n<ul>\\n<li>STUN server (RFC3489) : <a href=\\\"http://sourceforge.net/projects/stun\\\" rel=\\\"nofollow\\\">stund</a>  By c++ </li>\\n<li><p>STUN client (RFC3489) : <a href=\\\"http://code.google.com/p/pystun/\\\" rel=\\\"nofollow\\\">pystun</a>    By python</p></li>\\n<li><p>TURN server (RFC5766) : <a href=\\\"http://turnserver.sourceforge.net\\\" rel=\\\"nofollow\\\">turnserver</a> By c</p></li>\\n<li>TURN client (RFC5766) : <a href=\\\"https://github.com/node/turn-client\\\" rel=\\\"nofollow\\\">turn-client</a> By c and python </li>\\n</ul>\\n\\n<p>Note:\\nBecause TURN is the extension of new version STUN, the TURN server also support new STUN request by RFC5389 .</p>\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 102,\n        \"min\": -23,\n        \"max\": 8384,\n        \"num_unique_values\": 433,\n        \"samples\": [\n          199,\n          21,\n          4510\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 24987,\n        \"samples\": [\n          \"2009-02-09T14:30:34Z\",\n          \"2014-07-03T17:12:27Z\",\n          \"2015-09-30T15:09:36Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3702,\n        \"samples\": [\n          \"['python', 'python-imaging-library', 'dpi']\",\n          \"['python', 'django', 'django-admin', 'many-to-many']\",\n          \"['python', 'google-app-engine', 'web-crawler']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f3b6047f-322b-4a62-9d08-5d9273942e9b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_score</th>\n",
       "      <th>question_date</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_body</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>answer_date</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I find the full path to a font from it...</td>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>21</td>\n",
       "      <td>2008-08-02T15:11:16Z</td>\n",
       "      <td>497</td>\n",
       "      <td>&lt;p&gt;open up a terminal (Applications-&amp;gt;Utilit...</td>\n",
       "      <td>4</td>\n",
       "      <td>2008-08-02T16:56:53Z</td>\n",
       "      <td>['python', 'osx', 'fonts', 'photoshop']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I find the full path to a font from it...</td>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>21</td>\n",
       "      <td>2008-08-02T15:11:16Z</td>\n",
       "      <td>518</td>\n",
       "      <td>&lt;p&gt;I haven't been able to find anything that d...</td>\n",
       "      <td>2</td>\n",
       "      <td>2008-08-02T17:42:28Z</td>\n",
       "      <td>['python', 'osx', 'fonts', 'photoshop']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find the full path to a font from it...</td>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>21</td>\n",
       "      <td>2008-08-02T15:11:16Z</td>\n",
       "      <td>3040</td>\n",
       "      <td>&lt;p&gt;Unfortunately the only API that isn't depre...</td>\n",
       "      <td>12</td>\n",
       "      <td>2008-08-06T03:01:23Z</td>\n",
       "      <td>['python', 'osx', 'fonts', 'photoshop']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I find the full path to a font from it...</td>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>21</td>\n",
       "      <td>2008-08-02T15:11:16Z</td>\n",
       "      <td>195170</td>\n",
       "      <td>&lt;p&gt;There must be a method in Cocoa to get a li...</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-10-12T07:02:40Z</td>\n",
       "      <td>['python', 'osx', 'fonts', 'photoshop']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get a preview JPEG of a PDF on Windows?</td>\n",
       "      <td>502</td>\n",
       "      <td>&lt;p&gt;I have a cross-platform (Python) applicatio...</td>\n",
       "      <td>27</td>\n",
       "      <td>2008-08-02T17:01:58Z</td>\n",
       "      <td>536</td>\n",
       "      <td>&lt;p&gt;You can use ImageMagick's convert utility f...</td>\n",
       "      <td>9</td>\n",
       "      <td>2008-08-02T18:49:07Z</td>\n",
       "      <td>['python', 'windows', 'image', 'pdf']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3b6047f-322b-4a62-9d08-5d9273942e9b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f3b6047f-322b-4a62-9d08-5d9273942e9b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f3b6047f-322b-4a62-9d08-5d9273942e9b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-eb25e492-a98f-44d0-84bc-fb0d577131af\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb25e492-a98f-44d0-84bc-fb0d577131af')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-eb25e492-a98f-44d0-84bc-fb0d577131af button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               title  question_id  \\\n",
       "0  How can I find the full path to a font from it...          469   \n",
       "1  How can I find the full path to a font from it...          469   \n",
       "2  How can I find the full path to a font from it...          469   \n",
       "3  How can I find the full path to a font from it...          469   \n",
       "4            Get a preview JPEG of a PDF on Windows?          502   \n",
       "\n",
       "                                       question_body  question_score  \\\n",
       "0  <p>I am using the Photoshop's javascript API t...              21   \n",
       "1  <p>I am using the Photoshop's javascript API t...              21   \n",
       "2  <p>I am using the Photoshop's javascript API t...              21   \n",
       "3  <p>I am using the Photoshop's javascript API t...              21   \n",
       "4  <p>I have a cross-platform (Python) applicatio...              27   \n",
       "\n",
       "          question_date  answer_id  \\\n",
       "0  2008-08-02T15:11:16Z        497   \n",
       "1  2008-08-02T15:11:16Z        518   \n",
       "2  2008-08-02T15:11:16Z       3040   \n",
       "3  2008-08-02T15:11:16Z     195170   \n",
       "4  2008-08-02T17:01:58Z        536   \n",
       "\n",
       "                                         answer_body  answer_score  \\\n",
       "0  <p>open up a terminal (Applications-&gt;Utilit...             4   \n",
       "1  <p>I haven't been able to find anything that d...             2   \n",
       "2  <p>Unfortunately the only API that isn't depre...            12   \n",
       "3  <p>There must be a method in Cocoa to get a li...             1   \n",
       "4  <p>You can use ImageMagick's convert utility f...             9   \n",
       "\n",
       "            answer_date                                     tags  \n",
       "0  2008-08-02T16:56:53Z  ['python', 'osx', 'fonts', 'photoshop']  \n",
       "1  2008-08-02T17:42:28Z  ['python', 'osx', 'fonts', 'photoshop']  \n",
       "2  2008-08-06T03:01:23Z  ['python', 'osx', 'fonts', 'photoshop']  \n",
       "3  2008-10-12T07:02:40Z  ['python', 'osx', 'fonts', 'photoshop']  \n",
       "4  2008-08-02T18:49:07Z    ['python', 'windows', 'image', 'pdf']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv(dataset_file_path)\n",
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHmfuko-3yTK"
   },
   "source": [
    "# DataPreprocessor functions\n",
    "\n",
    "A data cleaning and transformation pipeline for StackOverflow question/answer data.\n",
    "\n",
    "\n",
    "### handle_missing_values(df)\n",
    "Fills missing values based on data type:\n",
    "- Numbers → 0\n",
    "- Text → empty string\n",
    "- Dates → minimum timestamp\n",
    "\n",
    "### clean_html(html_text)\n",
    "Returns `(cleaned_text, code_blocks)`:\n",
    "- Extracts `<code>` blocks (Removes code from the questions, so that better contextual embeddings can be learned and better retrieval results can be achieved)\n",
    "- Removes scripts/styles\n",
    "- Replaces URLs with \"[URL]\" (URLs also removed becasue it may reduces the retrival accuracy)\n",
    "- Cleans whitespace\n",
    "\n",
    "### transform_stackoverflow_data(df)\n",
    "Transforms data to question-centric format:\n",
    "- Groups by question_id\n",
    "- Keeps top 2 answers per question (On average 4-5 answers are available for each question, for simplicity i keep only top 2 answer on the basis of score.\n",
    "- Returns one row per question\n",
    "\n",
    "\n",
    "- Parallel processing with ThreadPoolExecutor (max number of workers can be configured depending on the system cores, it helps to precess the larger files parallely and and enhance the scalabilty of the pipeline)\n",
    "- Comprehensive error logging (logging is added os that any issue raised in the pipeline can be detected through the logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JOZpRJpzjAL-"
   },
   "outputs": [],
   "source": [
    "# Define possible data quality issues\n",
    "class DataQualityIssue(Enum):\n",
    "    MISSING_VALUE = \"missing_value\"\n",
    "    INVALID_FORMAT = \"invalid_format\"\n",
    "    INVALID_DATE = \"invalid_date\"\n",
    "    HTML_PARSING_ERROR = \"html_parsing_error\"\n",
    "    JSON_PARSING_ERROR = \"json_parsing_error\"\n",
    "\n",
    "\n",
    "# Data structure for quality reports\n",
    "@dataclass\n",
    "class DataQualityReport:\n",
    "    total_rows: int\n",
    "    missing_values: Dict[str, int]\n",
    "    invalid_formats: Dict[str, int]\n",
    "    cleaning_actions: List[str]\n",
    "\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"Initialize the preprocessor with common regex patterns and configs.\"\"\"\n",
    "        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        self.code_block_pattern = re.compile(r'<code>(.*?)</code>', re.DOTALL)\n",
    "        self.special_chars_pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "        # Setup logging configuration\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values based on column type and business rules.\n",
    "        Returns DataFrame with handled missing values.\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        df = df.copy()\n",
    "\n",
    "        # Fill missing values based on data type\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        text_cols = df.select_dtypes(include=['object']).columns\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "\n",
    "        # Handle numeric columns\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "        # Handle text columns\n",
    "        df[text_cols] = df[text_cols].fillna(\"\")\n",
    "\n",
    "        # Handle date columns\n",
    "        for col in date_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[col] = df[col].fillna(pd.Timestamp.min)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def clean_html(self, html_text: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        HTML cleaning with code block preservation and metadata extraction.\n",
    "        Returns cleaned text and list of extracted code blocks.\n",
    "        \"\"\"\n",
    "        if pd.isna(html_text):\n",
    "            return \"\", []\n",
    "\n",
    "        try:\n",
    "            # Extract code blocks before cleaning\n",
    "            code_blocks = self.code_block_pattern.findall(html_text)\n",
    "\n",
    "            # Use BeautifulSoup for HTML parsing\n",
    "            soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "\n",
    "            # Handle URLs - replace with placeholder\n",
    "            text = soup.get_text()\n",
    "            text = self.url_pattern.sub(\"[URL]\", text)\n",
    "\n",
    "            # Clean whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "            return text, code_blocks\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"HTML cleaning error: {str(e)}\")\n",
    "            return \"\", []\n",
    "\n",
    "    def validate_dates(self, date_str: str) -> bool:\n",
    "        \"\"\"Validate date string format and range.\"\"\"\n",
    "        try:\n",
    "            date = pd.to_datetime(date_str)\n",
    "            min_date = pd.Timestamp('2008-01-01')\n",
    "            max_date = pd.Timestamp.now()\n",
    "            return min_date <= date <= max_date\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def transform_stackoverflow_data(self,df):\n",
    "      \"\"\"\n",
    "      Transform StackOverflow data to have answers as JSON for each unique question.\n",
    "\n",
    "      Args:\n",
    "          df (pandas.DataFrame): Input DataFrame with question and answer data\n",
    "\n",
    "      Returns:\n",
    "          pandas.DataFrame: Transformed DataFrame with one row per question and answers as JSON\n",
    "      \"\"\"\n",
    "      # Create a list to store the transformed data\n",
    "      transformed_data = []\n",
    "\n",
    "      # Group by question_id\n",
    "      grouped = df.groupby('question_id')\n",
    "\n",
    "      for question_id, group in grouped:\n",
    "          # Get question details (will be same for all rows in group)\n",
    "          question_data = {\n",
    "              'question_id': int(question_id),\n",
    "              'title': group['title'].iloc[0],\n",
    "              'question_body': group['question_body'].iloc[0],\n",
    "              'question_score': group['question_score'].iloc[0],\n",
    "              'question_date': group['question_date'].iloc[0],\n",
    "              'tags': group['tags'].iloc[0]\n",
    "          }\n",
    "\n",
    "          # Create answers list\n",
    "          answers,answer_ids = [],[]\n",
    "          for _, row in group.iterrows():\n",
    "              answer = {\n",
    "                  'answer_score': row['answer_score'],\n",
    "                  'answer_id': int(row['answer_id']),\n",
    "                  'answer_body': row['answer_body'],\n",
    "                  'answer_date': row['answer_date']\n",
    "              }\n",
    "              answer_ids.append(str(row['answer_id']))\n",
    "              answers.append(answer)\n",
    "\n",
    "          # Add answers to question data\n",
    "          sorted_answer = list(sorted(answers, key=lambda x: x['answer_score'], reverse=True))\n",
    "          question_data['answers'] = sorted_answer[:2]\n",
    "          question_data[\"answer_ids\"] = answer_ids[:2]\n",
    "\n",
    "          transformed_data.append(question_data)\n",
    "\n",
    "      # Create new DataFrame from transformed data\n",
    "      result_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "      # Sort by question_id for consistency\n",
    "      result_df = result_df.sort_values('question_id').reset_index(drop=True)\n",
    "\n",
    "      return result_df\n",
    "\n",
    "    def clean_chunk(self, df_chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process a single chunk of data.\"\"\"\n",
    "        try:\n",
    "            # Handle missing values\n",
    "            df_chunk = self.handle_missing_values(df_chunk)\n",
    "\n",
    "            # Process text columns in parallel using ThreadPoolExecutor\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                for col in ['question_body', 'answer_body']:\n",
    "                    # Clean HTML and extract code blocks\n",
    "\n",
    "                    cleaned_results = list(executor.map(self.clean_html, df_chunk[col]))\n",
    "                    df_chunk[f'{col}_cleaned'] = [result[0] for result in cleaned_results]\n",
    "                    df_chunk[f'{col}_code_blocks'] = [result[1] for result in cleaned_results]\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    df_chunk[f'{col}_url_count'] = df_chunk[col].str.count(self.url_pattern)\n",
    "                    df_chunk[f'{col}_length'] = df_chunk[f'{col}_cleaned'].str.len()\n",
    "\n",
    "            # Process dates\n",
    "            date_cols = ['question_date', 'answer_date']\n",
    "            for col in date_cols:\n",
    "                df_chunk[col] = pd.to_datetime(df_chunk[col], errors='coerce')\n",
    "\n",
    "            # Create preprocessing metadata\n",
    "            df_chunk['preprocessing_metadata'] = df_chunk.apply(\n",
    "                lambda row: json.dumps({\n",
    "                    'question_urls': row['question_body_url_count'],\n",
    "                    'answer_urls': row['answer_body_url_count'],\n",
    "                    'question_length': row['question_body_length'],\n",
    "                    'answer_length': row['answer_body_length'],\n",
    "                    'has_code': bool(row['question_body_code_blocks'] or row['answer_body_code_blocks'])\n",
    "                }), axis=1\n",
    "            )\n",
    "\n",
    "            return df_chunk\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing chunk: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpykUPSKByyg"
   },
   "source": [
    "# StackOverflowPipeline\n",
    "\n",
    "## Initialization\n",
    "\n",
    "- **`__init__(self, db_path: str = 'stackoverflow.db', chunk_size: int = 10000)`**\n",
    "  - Initializes the pipeline with a database path and chunk size.\n",
    "  - Sets up logging to monitor operations. Persistant logging is added to keep the track of the processes in the pipeline, if something fishy we can track the logs in the CSV file.\n",
    "  - Configures the number of workers based on CPU cores.\n",
    "\n",
    "## Database Schema\n",
    "\n",
    "- **`create_database_schema(self)`**\n",
    "  - Schema Creation: The method defines the structure of the database, including tables, columns, and data types. It ensures that the database is set up to store questions, answers, and metadata in a way that minimizes redundancy and optimizes access speed.\n",
    "\n",
    "  - Optimized Settings: The schema is created with various SQLite settings that enhance performance:\n",
    "\n",
    "    - WAL Mode (Write-Ahead Logging): WAL mode improves the speed of read and write operations by logging changes before they are written to the database. This allows for concurrent reads and writes, increasing overall throughput and reducing the chances of database locks during heavy access.\n",
    "\n",
    "    - Synchronous Mode (Normal): In normal synchronous mode, SQLite ensures a balance between performance and data integrity. This reduces the write latency while still maintaining a reasonable level of data safety.\n",
    "\n",
    "    - Cache Configuration: The method sets an appropriate cache size to allow frequently accessed data to be stored in memory.\n",
    "\n",
    "\n",
    "## Indices\n",
    "\n",
    "- **`create_indices(self, conn)`**\n",
    "  - Creates database indices separately to improve insertion performance and query efficiency, it would be helpful if large data is pushed into DB\n",
    "  - Faster Lookups: It creates a data structure that speeds up queries that search or filter by question_id in the stackoverflow_posts table.\n",
    "\n",
    "## Data Transformation and Cleaning\n",
    "\n",
    "- **`transform_and_clean_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame`**\n",
    "  - Transforms and cleans a chunk of data in one pass.\n",
    "  - Uses parallel processing to clean HTML content.\n",
    "  - Calculates metrics like URL count and body length.\n",
    "  - Converts dates and stores answers and answer IDs as JSON strings.\n",
    "  - Creates preprocessing metadata.\n",
    "\n",
    "## Data Insertion\n",
    "\n",
    "- **`bulk_insert_chunk(self, conn: sqlite3.Connection, chunk: pd.DataFrame)`**\n",
    "  - Efficiently inserts a processed chunk into the database using bulk insertion.\n",
    "  - Prepares data for insertion and uses `executemany` for better performance in batch insertion.\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "- **`ingest_data(self, csv_path: str)`**\n",
    "  - Handles the entire data ingestion process using parallel processing.\n",
    "  - Creates the database schema and connection pool.\n",
    "  - Processes chunks of data in parallel and inserts them into the database.\n",
    "  - Creates indices after all data is loaded.\n",
    "\n",
    "## Query Execution\n",
    "\n",
    "- **`query_data(self, query: str) -> pd.DataFrame`**\n",
    "  - Executes a SQL query and returns the results as a DataFrame.\n",
    "  - Handles errors and logs execution details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hR0G05NVCby9"
   },
   "outputs": [],
   "source": [
    "class LogsCSVFormatter(logging.Formatter):\n",
    "    \"\"\"Custom formatter to output logs in CSV format\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output_fields = ['timestamp', 'level', 'module', 'function', 'message']\n",
    "\n",
    "    def format(self, record):\n",
    "        timestamp = datetime.fromtimestamp(record.created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return {\n",
    "            'timestamp': timestamp,\n",
    "            'level': record.levelname,\n",
    "            'module': record.module,\n",
    "            'function': record.funcName,\n",
    "            'message': record.getMessage()\n",
    "        }\n",
    "\n",
    "class CSVFileHandler(logging.FileHandler):\n",
    "    \"\"\"Custom file handler to write logs to CSV file\"\"\"\n",
    "    def __init__(self, filename, mode='a'):\n",
    "        super().__init__(filename, mode)\n",
    "        self.formatter = LogsCSVFormatter()\n",
    "\n",
    "        # Create CSV file with headers if it doesn't exist\n",
    "        if not os.path.exists(filename) or os.path.getsize(filename) == 0:\n",
    "            with open(filename, 'w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=self.formatter.output_fields)\n",
    "                writer.writeheader()\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            with open(self.baseFilename, 'a', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=self.formatter.output_fields)\n",
    "                writer.writerow(self.formatter.format(record))\n",
    "        except Exception:\n",
    "            self.handleError(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Jpxwl8n8Qh-V"
   },
   "outputs": [],
   "source": [
    "# Data Ingestion pipeline.\n",
    "class DataIngestionPipeline:\n",
    "    def __init__(self, db_path: str = 'stackoverflow.db', chunk_size: int = 10000, log_file: str = 'pipeline_logs.csv'):\n",
    "        \"\"\"Initialize the pipeline with configurable chunk size and logging.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "\n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not self.logger.handlers:\n",
    "            # Console handler with standard formatting\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            console_handler.setFormatter(console_formatter)\n",
    "\n",
    "            # CSV file handler\n",
    "            csv_handler = CSVFileHandler(log_file)\n",
    "\n",
    "            # Add both handlers\n",
    "            self.logger.addHandler(console_handler)\n",
    "            self.logger.addHandler(csv_handler)\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Configure the number of workers based on CPU cores\n",
    "        self.max_workers = max(1, 2)\n",
    "        self.logger.info(f\"Initialized pipeline with {self.max_workers} workers\")\n",
    "\n",
    "    def create_database_schema(self):\n",
    "        \"\"\"Create database schema with optimized settings.\"\"\"\n",
    "        self.logger.info(\"Creating database schema\")\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Enable WAL mode for better concurrent access\n",
    "            conn.execute('PRAGMA journal_mode=WAL')\n",
    "            conn.execute('PRAGMA synchronous=NORMAL')\n",
    "            conn.execute('PRAGMA cache_size=100000')\n",
    "            conn.execute('PRAGMA temp_store=MEMORY')\n",
    "\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS stackoverflow_posts (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    question_id INTEGER NOT NULL,\n",
    "                    title TEXT,\n",
    "                    question_body TEXT,\n",
    "                    question_score INTEGER,\n",
    "                    question_date TEXT,\n",
    "                    tags TEXT,\n",
    "                    answers TEXT,  -- Store answers as JSON array\n",
    "                    answer_ids TEXT,  -- Store answer IDs as JSON array\n",
    "                    question_body_cleaned TEXT,\n",
    "                    question_body_code_blocks TEXT,\n",
    "                    question_body_url_count INTEGER,\n",
    "                    question_body_length INTEGER,\n",
    "                    preprocessing_metadata TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "        self.logger.info(\"Database schema created successfully\")\n",
    "\n",
    "    def create_indices(self, conn):\n",
    "        \"\"\"Create indices separately for better insertion performance.\"\"\"\n",
    "        self.logger.info(\"Creating database indices\")\n",
    "        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_question_id ON stackoverflow_posts(question_id)\")\n",
    "        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_question_date ON stackoverflow_posts(question_date)\")\n",
    "        self.logger.info(\"Database indices created successfully\")\n",
    "\n",
    "    def transform_and_clean_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform and clean a chunk of data in one pass.\"\"\"\n",
    "        try:\n",
    "            chunk_size = len(chunk)\n",
    "            self.logger.debug(f\"Processing chunk of size {chunk_size}\")\n",
    "\n",
    "            # First transform the data to group by questions\n",
    "            transformed_df = self.preprocessor.transform_stackoverflow_data(chunk)\n",
    "\n",
    "            # Clean the transformed data\n",
    "            processed_df = transformed_df.copy()\n",
    "\n",
    "            # Process in parallel using ThreadPoolExecutor\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                # Clean question body HTML\n",
    "                cleaned_results = list(executor.map(\n",
    "                    self.preprocessor.clean_html,\n",
    "                    processed_df['question_body']\n",
    "                ))\n",
    "\n",
    "                processed_df['question_body_cleaned'] = [result[0] for result in cleaned_results]\n",
    "                processed_df['question_body_code_blocks'] = [result[1] for result in cleaned_results]\n",
    "\n",
    "            # Calculate metrics and process remaining fields\n",
    "            processed_df['question_body_url_count'] = processed_df['question_body'].apply(\n",
    "                lambda x: len(self.preprocessor.url_pattern.findall(x))\n",
    "            )\n",
    "            processed_df['question_body_length'] = processed_df['question_body_cleaned'].str.len()\n",
    "\n",
    "            # Convert dates\n",
    "            processed_df['question_date'] = pd.to_datetime(\n",
    "                processed_df['question_date'],\n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "            # Store answers and answer_ids as JSON strings\n",
    "            processed_df['answers'] = processed_df['answers'].apply(json.dumps)\n",
    "            processed_df['answer_ids'] = processed_df['answer_ids'].apply(json.dumps)\n",
    "\n",
    "            # Create preprocessing metadata\n",
    "            processed_df['preprocessing_metadata'] = processed_df.apply(\n",
    "                lambda row: json.dumps({\n",
    "                    'question_urls': row['question_body_url_count'],\n",
    "                    'question_length': row['question_body_length'],\n",
    "                    'has_code': bool(row['question_body_code_blocks']),\n",
    "                    'num_answers': len(json.loads(row['answers'])),\n",
    "                }), axis=1\n",
    "            )\n",
    "\n",
    "            self.logger.debug(f\"Successfully processed chunk of {chunk_size} rows\")\n",
    "            return processed_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing chunk: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def bulk_insert_chunk(self, conn: sqlite3.Connection, chunk: pd.DataFrame):\n",
    "        \"\"\"Efficiently insert a processed chunk into the database.\"\"\"\n",
    "        try:\n",
    "            chunk_size = len(chunk)\n",
    "            self.logger.debug(f\"Inserting chunk of {chunk_size} rows into database\")\n",
    "\n",
    "            # Prepare data for insertion\n",
    "            data = [\n",
    "                (\n",
    "                    int(row['question_id']),\n",
    "                    str(row['title']),\n",
    "                    str(row['question_body']),\n",
    "                    int(row['question_score']),\n",
    "                    str(row['question_date']),\n",
    "                    str(row['tags']),\n",
    "                    str(row['answers']),\n",
    "                    str(row['answer_ids']),\n",
    "                    str(row['question_body_cleaned']),\n",
    "                    json.dumps(row['question_body_code_blocks']),\n",
    "                    int(row['question_body_url_count']),\n",
    "                    int(row['question_body_length']),\n",
    "                    str(row['preprocessing_metadata'])\n",
    "                )\n",
    "                for _, row in chunk.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Bulk insert using executemany\n",
    "            conn.executemany(\"\"\"\n",
    "                INSERT INTO stackoverflow_posts (\n",
    "                    question_id, title, question_body, question_score,\n",
    "                    question_date, tags, answers, answer_ids,\n",
    "                    question_body_cleaned, question_body_code_blocks,\n",
    "                    question_body_url_count, question_body_length,\n",
    "                    preprocessing_metadata\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", data)\n",
    "\n",
    "            self.logger.debug(f\"Successfully inserted {chunk_size} rows into database\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error inserting chunk: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def ingest_data(self, csv_path: str):\n",
    "        \"\"\"Optimized data ingestion method using parallel processing.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting data ingestion from {csv_path}\")\n",
    "\n",
    "            # Create database schema\n",
    "            self.create_database_schema()\n",
    "\n",
    "            # Create connection pool\n",
    "            self.logger.info(f\"Creating database connection pool with {self.max_workers} connections\")\n",
    "            db_pool = []\n",
    "            for _ in range(self.max_workers):\n",
    "                conn = sqlite3.connect(self.db_path)\n",
    "                conn.execute('PRAGMA journal_mode=WAL')\n",
    "                conn.execute('PRAGMA synchronous=NORMAL')\n",
    "                db_pool.append(conn)\n",
    "\n",
    "            # Get total number of chunks for progress bar\n",
    "            total_chunks = sum(1 for _ in pd.read_csv(csv_path, chunksize=self.chunk_size))\n",
    "            self.logger.info(f\"Found {total_chunks} total chunks to process\")\n",
    "\n",
    "            # Process chunks in parallel\n",
    "            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                chunks = pd.read_csv(csv_path, chunksize=self.chunk_size)\n",
    "\n",
    "                # Submit all chunks for processing\n",
    "                future_to_chunk = {\n",
    "                    executor.submit(self.transform_and_clean_chunk, chunk): i\n",
    "                    for i, chunk in enumerate(chunks)\n",
    "                }\n",
    "\n",
    "                # Process results as they complete\n",
    "                with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "                    for future in concurrent.futures.as_completed(future_to_chunk):\n",
    "                        chunk_idx = future_to_chunk[future]\n",
    "                        try:\n",
    "                            processed_chunk = future.result()\n",
    "                            # Get a connection from the pool\n",
    "                            conn = db_pool[chunk_idx % len(db_pool)]\n",
    "                            # Insert the processed chunk\n",
    "                            self.bulk_insert_chunk(conn, processed_chunk)\n",
    "                            conn.commit()\n",
    "                            pbar.update(1)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error processing chunk {chunk_idx}: {str(e)}\")\n",
    "\n",
    "            # Close all connections\n",
    "            self.logger.info(\"Closing database connections\")\n",
    "            for conn in db_pool:\n",
    "                conn.close()\n",
    "\n",
    "            # Create indices after all data is loaded\n",
    "            self.logger.info(\"Creating final database indices\")\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                self.create_indices(conn)\n",
    "\n",
    "            self.logger.info(\"Completed data ingestion successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Fatal error during data ingestion: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def query_data(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute a query and return results as DataFrame.\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                return pd.read_sql_query(query, conn)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error executing query: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_post_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get basic statistical summary of posts.\"\"\"\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "                COUNT(*) as total_posts,\n",
    "                AVG(question_score) as avg_score,\n",
    "                COUNT(DISTINCT question_id) as unique_questions,\n",
    "                AVG(question_body_length) as avg_question_body_length\n",
    "            FROM stackoverflow_posts\n",
    "        \"\"\"\n",
    "        return self.query_data(query).iloc[0].to_dict()\n",
    "\n",
    "    def connect_db(self) -> sqlite3.Connection:\n",
    "        \"\"\"Create and return a database connection.\"\"\"\n",
    "        return sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xl1Udh8jROGs",
    "outputId": "c6df2691-8ffc-4307-c399-09a25cd467fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 04:19:38,626 - __main__ - INFO - Initialized pipeline with 2 workers\n",
      "INFO:__main__:Initialized pipeline with 2 workers\n",
      "2024-11-05 04:19:38,630 - __main__ - INFO - Starting data ingestion from /content/python_stackover_flow_dataset.csv\n",
      "INFO:__main__:Starting data ingestion from /content/python_stackover_flow_dataset.csv\n",
      "2024-11-05 04:19:38,631 - __main__ - INFO - Creating database schema\n",
      "INFO:__main__:Creating database schema\n",
      "2024-11-05 04:19:38,647 - __main__ - INFO - Database schema created successfully\n",
      "INFO:__main__:Database schema created successfully\n",
      "2024-11-05 04:19:38,657 - __main__ - INFO - Creating database connection pool with 2 connections\n",
      "INFO:__main__:Creating database connection pool with 2 connections\n",
      "2024-11-05 04:19:39,151 - __main__ - INFO - Found 3 total chunks to process\n",
      "INFO:__main__:Found 3 total chunks to process\n",
      "Processing chunks: 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]\n",
      "2024-11-05 04:19:45,325 - __main__ - INFO - Closing database connections\n",
      "INFO:__main__:Closing database connections\n",
      "2024-11-05 04:19:45,332 - __main__ - INFO - Creating final database indices\n",
      "INFO:__main__:Creating final database indices\n",
      "2024-11-05 04:19:45,337 - __main__ - INFO - Creating database indices\n",
      "INFO:__main__:Creating database indices\n",
      "2024-11-05 04:19:45,371 - __main__ - INFO - Database indices created successfully\n",
      "INFO:__main__:Database indices created successfully\n",
      "2024-11-05 04:19:45,375 - __main__ - INFO - Completed data ingestion successfully\n",
      "INFO:__main__:Completed data ingestion successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    if os.path.exists('stackoverflow.db'):\n",
    "        os.remove('stackoverflow.db')\n",
    "\n",
    "    # Initialize with log file\n",
    "    data_ingestion_pipeline = DataIngestionPipeline(\n",
    "        db_path='stackoverflow.db',\n",
    "        chunk_size=10000,\n",
    "        log_file='data_ingestion_logs.csv'\n",
    "    )\n",
    "\n",
    "    data_ingestion_pipeline.ingest_data(dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cxbh3PyHpASD",
    "outputId": "9fa5f107-b954-40c5-84d0-e2c1074b143b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_posts': 5280.0,\n",
       " 'avg_score': 35.29450757575758,\n",
       " 'unique_questions': 5279.0,\n",
       " 'avg_question_body_length': 705.8554924242425}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion_pipeline.get_post_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ILakAbxPv4X"
   },
   "source": [
    "## Data Injestion to Vector Database(ChromaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWwSY9QGHTwH"
   },
   "source": [
    "# VectorIngestion pipeline\n",
    "\n",
    "The `VectorIngestion` class is designed to initialize and manage the ingestion of vector embeddings into a vector database.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "### `__init__(self, open_source_mode)`\n",
    "The constructor initializes the vector ingestion pipeline.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `open_source_mode` (bool): Determines whether to use open-source embeddings or VoyageAI embeddings.\n",
    "\n",
    "- **Steps:**\n",
    "  1. **Retrieve API Key:** Attempts to fetch the `VOYAGEAI_API_KEY` from user data.\n",
    "  2. **Set Mode:** Stores the `open_source_mode` flag.\n",
    "  3. **Initialize Hugging Face Client:** Creates an inference client for Hugging Face using a specific API key.\n",
    "  4. **Set Index Name:** Sets the index name for the vector database to `\"chroma_vectorDB\"`.\n",
    "  5. **Model and Collection Setup:**\n",
    "     - If `open_source_mode` is `True`:\n",
    "       - Downloads the Hugging Face embeddings model `\"all-mpnet-base-v2\"`.\n",
    "       - Sets the collection name to `\"hf_collection\"`.\n",
    "     - If `open_source_mode` is `False`:\n",
    "       - Sets the VoyageAI API key.\n",
    "       - Initializes the VoyageAI embeddings model with the provided API key and model `\"voyage-large-2-instruct\"`.\n",
    "       - Sets the collection name to `\"voyageai_collection\"`.\n",
    "  6. **Initialize ChromaDB:**\n",
    "     - Creates a Chroma vector store instance with the specified collection name, embedding function, and a directory for persisting data locally.\n",
    "\n",
    "\n",
    "## Data Parsing\n",
    "\n",
    "### `data_parsing(self, df)`\n",
    "This method processes a DataFrame to extract documents, metadata, and unique IDs for ingestion into the vector database.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `df` (DataFrame): The input data to be processed.\n",
    "\n",
    "- **Steps:**\n",
    "  1. **Initialize Lists:** Creates empty lists for documents, metadata, and unique IDs.\n",
    "  2. **Iterate Over DataFrame Rows:**\n",
    "     - For each row, combines the title and question body to form a document. Joining Title and question give contextual details to the question it learns better contextual embeddings.\n",
    "     - Constructs a metadata dictionary with question ID, tags, title, question body, and answer IDs.\n",
    "     - Generates a unique ID for each document using `uuid`.\n",
    "\n",
    "\n",
    "## Embedding Generation\n",
    "\n",
    "### `generate_embeddings(self, documents, input_type)`\n",
    "Generates embeddings for a list of documents using the specified input type.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `documents` (list): The documents to generate embeddings for.\n",
    "  - `input_type` (str): The type of input for the embedding model.\n",
    "\n",
    "- **Steps:**\n",
    "\n",
    "  1. **Set Batch Size:** Defines the batch size for processing documents.\n",
    "  2. **Initialize Embeddings List:** Creates an empty list to store embeddings.\n",
    "  3. **Iterate Over Documents in Batches:**\n",
    "     - Generates embeddings for each batch using the VoyageAI model.\n",
    "     - Appends the generated embeddings to the embeddings list.\n",
    "  4. **Return Embeddings:** Outputs the list of embeddings.\n",
    "\n",
    "\n",
    "## Data Insertion to Vector Database\n",
    "\n",
    "### `data_insertion_to_VectorDB(self, documents, metadatas, ids)`\n",
    "Inserts documents, metadata, and unique IDs into the vector database.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `documents` (list): The documents to insert.\n",
    "  - `metadatas` (list): The metadata associated with the documents.\n",
    "  - `ids` (list): The unique IDs for the documents.\n",
    "\n",
    "- **Steps:**\n",
    "  1. **Create Document Objects:** Constructs `Document` objects for each document, combining content, metadata, and ID.\n",
    "  2. **Add Documents to Vector Store:** Inserts the documents into the vector store using the `add_documents` method.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hXTIZ-m6J4zc"
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "import voyageai\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW6KL5-XhPHO"
   },
   "outputs": [],
   "source": [
    "class VectorIngestion:\n",
    "  def __init__(self,open_source_mode):\n",
    "\n",
    "    self.VOYAGEAI_API_KEY = VoyageAI_API_KEY\n",
    "    self.open_source_mode = open_source_mode\n",
    "    self.hf_client = InferenceClient(api_key=hf_client_API_KEY)\n",
    "    self.index_name = \"chroma_vectorDB\"\n",
    "\n",
    "\n",
    "    if self.open_source_mode:\n",
    "      #dowload model and tokenizer\n",
    "      embeddings_model = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "      collection_name = \"hf_collection\"\n",
    "\n",
    "    else:\n",
    "      # Initialize Voyageai\n",
    "      embeddings_model = VoyageAIEmbeddings(api_key = self.VOYAGEAI_API_KEY ,  model=\"voyage-large-2-instruct\")\n",
    "      collection_name = \"voyageai_collection\"\n",
    "\n",
    "    # #initialize chromaDB\n",
    "    self.vector_store = Chroma(\n",
    "      collection_name=collection_name,\n",
    "      embedding_function=embeddings_model,\n",
    "      persist_directory=\"./chroma_db\",  # Where to save data locally, remove if not necessary\n",
    "    )\n",
    "\n",
    "    print(f\"ChromaDB Vector Store Intialized...\\n\")\n",
    "    print(f\"Index named {self.index_name} created...\\n\")\n",
    "    print(f\"Embedding Model Intialized...\\n\")\n",
    "    print(f\"HuggingFace LLM Client Intialized...\\n\")\n",
    "\n",
    "\n",
    "  def data_parsing(self,df):\n",
    "    print(f\"creating the documents, metadata and uniques ids to insert into vector database\")\n",
    "    docs,metadatas,ids = [],[],[]\n",
    "    for id, row in df.iterrows():\n",
    "\n",
    "      docs.append(row['title'] + '\\n\\n' + row[\"question_body\"])\n",
    "      metadata = {\"question_id\":row[\"question_id\"],\n",
    "                  \"tags\":row['tags'],\n",
    "                  \"title\":row[\"title\"],\n",
    "                  \"question_body\":row[\"question_body\"],\n",
    "                  \"answer_ids\" : json.dumps(row[\"answer_ids\"])\n",
    "                  }\n",
    "      metadatas.append(metadata)\n",
    "      ids.append(str(uuid.uuid4()))\n",
    "    print(f\"length of uniques question: {len(docs),len(metadatas),len(ids)}\")\n",
    "    return docs,metadatas,ids\n",
    "\n",
    "\n",
    "  def generate_embeddings(self,documents,input_type):\n",
    "      print(f\"\\nGenerating the embeddings of the {len(documents)} documents...\")\n",
    "      voyageai.api_key = self.VOYAGEAI_API_KEY\n",
    "      vo = voyageai.Client()\n",
    "      # Generate embeddings\n",
    "      batch_size = 128\n",
    "      embeddings = []\n",
    "\n",
    "      for i in tqdm(range(0, len(documents), batch_size)):\n",
    "          embeddings += vo.embed(\n",
    "              documents[i:i + batch_size], model=\"voyage-large-2-instruct\", input_type=input_type\n",
    "          ).embeddings\n",
    "\n",
    "      return embeddings\n",
    "\n",
    "\n",
    "  def data_insertion_to_vectordb(\n",
    "            self,\n",
    "            documents: List[str],\n",
    "            metadatas: List[Dict[str, Any]],\n",
    "            ids: List[str],\n",
    "            batch_size: int = None\n",
    "        ) -> bool:\n",
    "            \"\"\"\n",
    "            Insert documents into vector database in batches.\n",
    "\n",
    "            Args:\n",
    "                documents: List of text documents\n",
    "                metadatas: List of metadata dictionaries\n",
    "                ids: List of document IDs\n",
    "                batch_size: Optional batch size override\n",
    "\n",
    "            Returns:\n",
    "                bool: True if insertion was successful\n",
    "            \"\"\"\n",
    "            if batch_size is None:\n",
    "                batch_size = self.batch_size\n",
    "\n",
    "            total_docs = len(documents)\n",
    "            successful_insertions = 0\n",
    "\n",
    "            # Process in batches with progress bar\n",
    "            with tqdm(total=total_docs, desc=\"Inserting documents\") as pbar:\n",
    "                for i in range(0, total_docs, batch_size):\n",
    "                    # Get batch slices\n",
    "                    batch_docs = documents[i:i + batch_size]\n",
    "                    batch_metadata = metadatas[i:i + batch_size]\n",
    "                    batch_ids = ids[i:i + batch_size]\n",
    "\n",
    "                    # Create Document objects for the batch\n",
    "                    batch_documents = [\n",
    "                        Document(\n",
    "                            page_content=doc,\n",
    "                            metadata=metadata,\n",
    "                            id=doc_id\n",
    "                        )\n",
    "                        for doc, metadata, doc_id in zip(batch_docs, batch_metadata, batch_ids)\n",
    "                    ]\n",
    "\n",
    "                    try:\n",
    "                        # Insert batch into vector store\n",
    "                        resp = self.vector_store.add_documents(\n",
    "                            documents=batch_documents,\n",
    "                            ids=batch_ids\n",
    "                        )\n",
    "                        successful_insertions += len(resp)\n",
    "                        pbar.update(len(batch_docs))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error inserting batch {i//batch_size}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "            print(f\"Successfully inserted {successful_insertions}/{total_docs} documents\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bVbNMkYgqDb"
   },
   "source": [
    "Open source model is faster because it a small model but prefer to keep ```open_source_flag=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4blq8aJdOQq7",
    "outputId": "4a193801-369f-4b81-a100-bee433e4937f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB Vector Store Intialized...\n",
      "\n",
      "Index named chroma_vectorDB created...\n",
      "\n",
      "Embedding Model Intialized...\n",
      "\n",
      "HuggingFace LLM Client Intialized...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "open_source_mode = True\n",
    "inject_vectors = VectorIngestion(open_source_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KKsUcGpkJEM8"
   },
   "outputs": [],
   "source": [
    "#Fetching all the records of python_stackOverFlow as a dataframe from the sqlite database\n",
    "sql_query = \"select * from stackoverflow_posts\"\n",
    "df = data_ingestion_pipeline.query_data(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w63zII3EUGua"
   },
   "source": [
    "This cell will generate embeddings of the whole dataset and store in vector DB, it will take around 1 and half minute to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOG5_yJkDHca",
    "outputId": "8accb5f6-a82f-462e-a6cd-39b2fbcdc088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the documents, metadata and uniques ids to insert into vector database\n",
      "length of uniques question: (5280, 5280, 5280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting documents: 100%|██████████| 5280/5280 [01:55<00:00, 45.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted 5280/5280 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#pass the dataframe to data parsing function, which parse the data and generate embeddings store it in the Vector database.\n",
    "batch_size = 256\n",
    "documents,metadatas,ids = inject_vectors.data_parsing(df)\n",
    "inject_vectors.data_insertion_to_vectordb(documents,metadatas,ids,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMG4NyOkQNrF"
   },
   "source": [
    "#Inference Pipeline (Query and Retrieve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DthU6RIsN8j6"
   },
   "source": [
    "## Initialization\n",
    "\n",
    "### `__init__(self, ingestion_pipeline, open_source_mode)`\n",
    "Initializes the inference class by extending the `VectorIngestion` class. Sets up logging to monitor operations. Persistant logging is added to keep the track of the processes in the pipeline, if something fishy we can track the logs in the CSV file.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `ingestion_pipeline`: The data ingestion pipeline.\n",
    "  - `open_source_mode` (bool): Determines whether to use open-source embeddings or VoyageAI embeddings.\n",
    "\n",
    "- **Attributes:**\n",
    "  - `ingestion_pipeline`: Stores the ingestion pipeline.\n",
    "  - `model_name`: The name of the language model to be used (`\"mistralai/Mistral-Nemo-Instruct-2407\"`).\n",
    "  - `sql_query`: A template SQL query to fetch answers from the database based on question IDs.\n",
    "  - `prompt`: A pre-defined prompt template for generating responses from the LLM.\n",
    "\n",
    "\n",
    "## Retriever\n",
    "\n",
    "### `retriever(self, query, verbose)`\n",
    "Retrieves relevant context from the vector database based on the user's query.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `query` (str): The user's query.\n",
    "  - `verbose` (bool): If `True`, prints additional debugging information.\n",
    "\n",
    "- **Steps:**\n",
    "  1. **Search Vector Store:** Calls the vector store to retrieve documents similar to the query.\n",
    "  2. **Extract Metadata:** Extracts and stores metadata from the results.\n",
    "  3. **Execute SQL Query:** Replaces placeholders in the SQL query and fetches answers.\n",
    "  4. **Aggregate Context:** Aggregates relevant answers into a single context string.\n",
    "  5. **Print Debug Info:** If `verbose` is `True`, prints detailed information about the retrieved documents and context.\n",
    "\n",
    "- **Returns:** Lists of question IDs, titles, tags, question bodies, scores, answer IDs, and the relevant context.\n",
    "\n",
    "\n",
    "\n",
    "## LLM Call\n",
    "\n",
    "### `llm_call(self, query, verbose, stream)`\n",
    "Generates a response to the user's query using a language model.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `query` (str): The user's query.\n",
    "  - `verbose` (bool): If `True`, prints additional debugging information.\n",
    "  - `stream` (bool): If `True`, streams the response.\n",
    "\n",
    "- **Steps:**\n",
    "  1. **Retrieve Context:** Calls the `retriever` method to get relevant context.\n",
    "  2. **Prepare Prompt:** Replaces placeholders in the prompt template with the query and context.\n",
    "  3. **Send LLM Request:** Sends the prepared prompt to the language model.\n",
    "  4. **Print Response:** Prints the model's response, optionally streaming it if `stream` is `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "m31yqu2Bfu3h"
   },
   "outputs": [],
   "source": [
    "class InferenceClass(VectorIngestion):\n",
    "    def __init__(self, ingestion_pipeline, open_source_mode, log_file: str = 'logs/inference_logs.csv'):\n",
    "        \"\"\"Initialize the inference pipeline with logging.\"\"\"\n",
    "        super().__init__(open_source_mode)\n",
    "\n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)  # Set level before adding handlers\n",
    "\n",
    "        # Remove any existing handlers to avoid duplicates\n",
    "        if self.logger.handlers:\n",
    "            for handler in self.logger.handlers:\n",
    "                self.logger.removeHandler(handler)\n",
    "\n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(console_formatter)\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # CSV file handler\n",
    "        csv_handler = CSVFileHandler(log_file)\n",
    "        csv_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Add both handlers\n",
    "        self.logger.addHandler(console_handler)\n",
    "        self.logger.addHandler(csv_handler)\n",
    "\n",
    "        self.ingestion_pipeline = ingestion_pipeline\n",
    "        self.model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "        self.logger.info(f\"Initialized inference pipeline with model: {self.model_name}\")\n",
    "\n",
    "\n",
    "        self.sql_query = f\"\"\"\n",
    "        SELECT answers\n",
    "        FROM stackoverflow_posts\n",
    "        WHERE question_id IN (_question_ids_);\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = \"\"\"You are a helpful assistant. Your task is to answer the user's query related to Python based on the provided context. The context consists of answers from developers to questions asked on Stack Overflow.\n",
    "\n",
    "Instructions:\n",
    "Cohesive and Clear Response: Ensure your answer is well-organized, easy to understand, and directly addresses the user's query.\n",
    "Include Code Examples: If a relevant code example is available in the provided context, include it in your response.\n",
    "Contextual Limitation: If the answer to the query is not present in the provided context or context is empty, must respond with: \"Sorry, the provided context does not contain an answer to the query.\"\n",
    "\n",
    "If the answer to the query is not present in the provided context, do not add anything from your knowledge.\n",
    "Your answer should be less than 500 words.\n",
    "\n",
    "Query:\n",
    "__query__\n",
    "\n",
    "Context:\n",
    "__context__\n",
    "\"\"\"\n",
    "\n",
    "    def retriever(self, query, verbose):\n",
    "        \"\"\"Retrieve relevant context from vector store.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting retrieval for query: {query[:100]}...\")  # Log first 100 chars of query\n",
    "\n",
    "            question_ids = []\n",
    "            titles = []\n",
    "            tags = []\n",
    "            question_bodies = []\n",
    "            s_scores = []\n",
    "            answer_ids = []\n",
    "            relevant_context = []\n",
    "\n",
    "            self.logger.info(\"Querying chromaDB vector stores...\")\n",
    "            results = self.vector_store.similarity_search(query, k=3)\n",
    "\n",
    "            for res in results:\n",
    "                metadata = res.metadata\n",
    "                question_ids.append(metadata['question_id'])\n",
    "                titles.append(metadata['title'])\n",
    "                question_bodies.append(metadata['question_body'])\n",
    "                tags.append(metadata['tags'])\n",
    "\n",
    "            self.logger.debug(f\"Retrieved {len(question_ids)} documents from vector store\")\n",
    "\n",
    "            sql_query = self.sql_query.replace(\"_question_ids_\", str(','.join([f'{i}' for i in question_ids])))\n",
    "            self.logger.debug(\"Executing SQL query to fetch answers\")\n",
    "\n",
    "            answers = self.ingestion_pipeline.query_data(sql_query)['answers'].to_list()\n",
    "            relevant_context = \"\\n\\n\".join([ans['answer_body'] for answer in answers for ans in json.loads(answer)[:2]])\n",
    "\n",
    "            total_inputs_vectors = len(question_ids)\n",
    "            self.logger.info(f\"Retrieved {total_inputs_vectors} relevant documents\")\n",
    "\n",
    "            if verbose:\n",
    "                self.logger.info(f\"Question IDs: {question_ids}\")\n",
    "                self.logger.debug(f\"Titles: {titles}\")\n",
    "                self.logger.debug(f\"Context length: {len(relevant_context)} characters\")\n",
    "                self.logger.debug(f\"Context: {relevant_context}\")\n",
    "                print(f\"\\n\\nContext: {relevant_context}\\n\\n\")\n",
    "\n",
    "            return question_ids, titles, tags, question_bodies, s_scores, answer_ids, relevant_context\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in retriever: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def llm_call(self, query, verbose, stream):\n",
    "        \"\"\"Make LLM API call with retrieved context.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Starting LLM call process\")\n",
    "\n",
    "            # Get retrieved context\n",
    "            retrieval_start = datetime.now()\n",
    "            question_ids, titles, tags, question_bodies, s_scores, answer_ids, relevant_answers = self.retriever(query, verbose)\n",
    "            retrieval_time = (datetime.now() - retrieval_start).total_seconds()\n",
    "            self.logger.info(f\"Context retrieval completed in {retrieval_time:.2f} seconds\")\n",
    "\n",
    "            # Prepare prompt\n",
    "            prompt = self.prompt.replace(\"__query__\", query).replace(\"__context__\", str(relevant_answers))\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "            # Make LLM call\n",
    "            self.logger.info(f\"Making LLM API call to {self.model_name}\")\n",
    "            llm_start = datetime.now()\n",
    "            response = self.hf_client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                max_tokens=600,\n",
    "                stream=stream\n",
    "            )\n",
    "\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"-\"*180)\n",
    "            print(f\"\\n\\n\\nQuestion: {query}\\n\\nAnswer: \")\n",
    "\n",
    "            # Handle streaming vs non-streaming response\n",
    "            if stream:\n",
    "                response_text = \"\"\n",
    "                for chunk in response:\n",
    "                    chunk_text = chunk.choices[0].delta.content\n",
    "                    response_text += chunk_text\n",
    "                    print(chunk_text, end=\"\")\n",
    "            else:\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "\n",
    "            llm_time = (datetime.now() - llm_start).total_seconds()\n",
    "            self.logger.info(f\"LLM response completed in {llm_time:.2f} seconds\")\n",
    "\n",
    "            # Log response metadata\n",
    "            self.logger.debug({\n",
    "                \"query_length\": len(query),\n",
    "                \"context_length\": len(relevant_answers),\n",
    "                \"response_length\": len(response_text),\n",
    "                \"retrieval_time\": retrieval_time,\n",
    "                \"llm_time\": llm_time\n",
    "            })\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"-\"*180)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in LLM call: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Bpj3K0XhjM7",
    "outputId": "89bf5b08-b9d8-4884-e603-39b0211c9db1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 04:21:53,037 - __main__ - INFO - Initialized inference pipeline with model: mistralai/Mistral-Nemo-Instruct-2407\n",
      "INFO:__main__:Initialized inference pipeline with model: mistralai/Mistral-Nemo-Instruct-2407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB Vector Store Intialized...\n",
      "\n",
      "Index named chroma_vectorDB created...\n",
      "\n",
      "Embedding Model Intialized...\n",
      "\n",
      "HuggingFace LLM Client Intialized...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize the inference class\n",
    "inference_instance = InferenceClass(\n",
    "    ingestion_pipeline=data_ingestion_pipeline,\n",
    "    open_source_mode=open_source_mode,\n",
    "    log_file='inference_logs.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk739wVsTMIY"
   },
   "source": [
    "**For single query run this cell and pass the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWQF9tJ7SdI4",
    "outputId": "3bf4f355-180d-4e79-bd57-f064ce37cc0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 04:22:26,653 - __main__ - INFO - Starting LLM call process\n",
      "INFO:__main__:Starting LLM call process\n",
      "2024-11-05 04:22:26,657 - __main__ - INFO - Starting retrieval for query: what GIL lock in python?...\n",
      "INFO:__main__:Starting retrieval for query: what GIL lock in python?...\n",
      "2024-11-05 04:22:26,660 - __main__ - INFO - Querying chromaDB vector stores...\n",
      "INFO:__main__:Querying chromaDB vector stores...\n",
      "2024-11-05 04:22:26,744 - __main__ - INFO - Retrieved 3 relevant documents\n",
      "INFO:__main__:Retrieved 3 relevant documents\n",
      "2024-11-05 04:22:26,751 - __main__ - INFO - Question IDs: [105095, 105095, 105095]\n",
      "INFO:__main__:Question IDs: [105095, 105095, 105095]\n",
      "2024-11-05 04:22:26,755 - __main__ - INFO - Context retrieval completed in 0.10 seconds\n",
      "INFO:__main__:Context retrieval completed in 0.10 seconds\n",
      "2024-11-05 04:22:26,759 - __main__ - INFO - Making LLM API call to mistralai/Mistral-Nemo-Instruct-2407\n",
      "INFO:__main__:Making LLM API call to mistralai/Mistral-Nemo-Instruct-2407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Context: <p>You will still need locks if you share state between threads. The GIL only protects the interpreter internally. You can still have inconsistent updates in your own code.</p>\n",
      "\n",
      "<p>For example:</p>\n",
      "\n",
      "<pre><code>#!/usr/bin/env python\n",
      "import threading\n",
      "\n",
      "shared_balance = 0\n",
      "\n",
      "class Deposit(threading.Thread):\n",
      "    def run(self):\n",
      "        for _ in xrange(1000000):\n",
      "            global shared_balance\n",
      "            balance = shared_balance\n",
      "            balance += 100\n",
      "            shared_balance = balance\n",
      "\n",
      "class Withdraw(threading.Thread):\n",
      "    def run(self):\n",
      "        for _ in xrange(1000000):\n",
      "            global shared_balance\n",
      "            balance = shared_balance\n",
      "            balance -= 100\n",
      "            shared_balance = balance\n",
      "\n",
      "threads = [Deposit(), Withdraw()]\n",
      "\n",
      "for thread in threads:\n",
      "    thread.start()\n",
      "\n",
      "for thread in threads:\n",
      "    thread.join()\n",
      "\n",
      "print shared_balance\n",
      "</code></pre>\n",
      "\n",
      "<p>Here, your code can be interrupted between reading the shared state (<code>balance = shared_balance</code>) and writing the changed result back (<code>shared_balance = balance</code>), causing a lost update. The result is a random value for the shared state.</p>\n",
      "\n",
      "<p>To make the updates consistent, run methods would need to lock the shared state around the read-modify-write sections (inside the loops) or have <a href=\"http://en.wikipedia.org/wiki/Software_transactional_memory\">some way to detect when the shared state had changed since it was read</a>.</p>\n",
      "\n",
      "\n",
      "<p>No - the GIL just protects python internals from multiple threads altering their state.  This is a very low-level of locking, sufficient only to keep python's own structures in a consistent state.  It doesn't cover the <em>application</em> level locking you'll need to do to cover thread safety in your own code.</p>\n",
      "\n",
      "<p>The essence of locking is to ensure that a particular <em>block</em> of code is only executed by one thread.  The GIL enforces this for blocks the size of a single bytecode, but usually you want the lock to span a larger block of code than this.</p>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Question: what GIL lock in python?\n",
      "\n",
      "Answer: \n",
      "The GIL (Global Interpreter Lock) in Python is a mechanism used to synchronize access to Python objects, preventing multiple native threads from executing Python bytecodes at once. It's a low-level lock that protects Python's internal data structures from being concurrently modified by multiple threads.\n",
      "\n",
      "Here's a simple breakdown:\n",
      "\n",
      "1. **What it does**: The GIL ensures that only one thread executes Python bytecodes at a time. This is crucial for maintaining the internal consistency of Python's interpreter and its data structures.\n",
      "\n",
      "2. **What it doesn't do**: The GIL doesn't cover application-level locking. It doesn't protect your own code's shared state from being concurrently modified by multiple threads. In other words, it won't prevent race conditions or inconsistent updates in your own code.\n",
      "\n",
      "Here's an example from the provided context that demonstrates this:\n",
      "\n",
      "```python\n",
      "import threading\n",
      "\n",
      "shared_balance = 0\n",
      "\n",
      "class Deposit(threading.Thread):\n",
      "    def run(self):\n",
      "        for _ in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 04:22:39,802 - __main__ - INFO - LLM response completed in 13.04 seconds\n",
      "INFO:__main__:LLM response completed in 13.04 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is python's list comprehension?\"\n",
    "# query = \"how to sort python dictionary on the basis of values?\"\n",
    "# query = \"what are the generators in python?\"\n",
    "# query = \"what are the OOP concepts in python?\"\n",
    "# query = \"what is the difference between lists and tuples?\"\n",
    "query = \"what GIL lock in python?\"\n",
    "# query = \"How to reverse a list in Python?\"\n",
    "# query = \"what the whether like in new york\"\n",
    "\n",
    "verbose = True\n",
    "stream = True\n",
    "inference_instance.llm_call(query,verbose,stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_9rcXF7TV-K"
   },
   "source": [
    "**To run the inference in a loop run this cell and enter the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5BujDqrOIk-4"
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "stream = True\n",
    "\n",
    "# response_df = inference.retriever(query,verbose)\n",
    "while True:\n",
    "  query = input(\"Please enter your python related query...\")\n",
    "  inference_instance.llm_call(query,verbose,stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PjDn2SnUAMt"
   },
   "source": [
    "## Future Improvements:\n",
    "\n",
    "\n",
    "  \n",
    "- **Enhanced Embeddings:** Utilize better pre-trained embedding models to obtain more accurate contextual embeddings for queries and data, improving result accuracy.\n",
    "  \n",
    "- **Serverless Vector Databases:** Explore serverless vector databases such as Pinecone and Weaviate to achieve better latency performance and to get faster batch upsertion.\n",
    "  \n",
    "- **Hybrid Search Integration:** Integrate hybrid search techniques to enhance retrieval accuracy.\n",
    "  \n",
    "- **Advanced LLMs:** Use more robust large language models (LLMs) to generate more accurate and reliable query responses.\n",
    "\n",
    "- **Persistent Storage:** Implement other databases like PostgreSQL for more efficient and persistent storage solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZADwVoN5X0V5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hylychat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
